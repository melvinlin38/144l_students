---
title: "DADA2 144L_2018"
author: "Melvin Lin"
date: "11/11/2020"
output: github_document
---

# Install and Load DADA2 and ShortRead from Bioconductor 
```{r}
#install.packages("devtools")
#library("devtools")
#devtools::install_github("benjjneb/dada2", ref="v1.16")
```

```{r}
#library("devtools")
#devtools::install_github("benjjneb/dada2")
```

```{r}
library(dada2)
library(ShortRead)
# install.packages("dplyr")
# install.packages("tidyverse")
library(tidyverse)
```

```{r}
path1 <- "~/GITHUB/144l_students/Input_Data/week5/EEMB144L_2018_fastq/"

#storing the names of forward and reverse files as lists
fnFs <- list.files(path1, pattern = "_R1_001.fastq", full.names = TRUE)
fnRs <- list.files(path1, pattern = "_R2_001.fastq", full.names = TRUE)

```

```{r}
#store the forward and reverse primers
FWD = "GTGYCAGCMGCCGCGGTAA"
REV = "GGACTACNVGGGTWTCTAAT"

allOrients <- function(primer) {
  require(Biostrings)
  dna <- DNAString(primer)
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna),
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))
}

#store the fwd and reverse orientations separately 
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)

#view the orientations of the primers
FWD.orients
REV.orients
```

# Search for Primers

```{r}
primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]),
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]),
      REV.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]),
      REV.ReverseReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]))

# All numbers are 0, so there are no primers in the sequence
```

# Inspect read quality profiles

## Forward reads

```{r fig.height=10, fig.width=12}
plotQualityProfile((fnFs[1:12]))
```

## Reverse reads
```{r fig.height=10, fig.width=12}
plotQualityProfile(fnRs[1:12])
```

# Filtering and Trimming
```{r}
#Get the sample names
#define the basename of the FnFs as the first part of each fastQ file name until "_L"
#apply this to all samples
sample.names <- sapply(strsplit(basename(fnFs), "_L"), `[`, 1)
sample.names
#create a "filtered" folder in the working directory as a place to put all new filtered fastQ files

filt_path <- file.path(path1,"filtered")
#add the appropriate designation string to any new files made that will be put into the "filtered" folder
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq"))
```

```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen = c(240,150), maxN = 0, maxEE = c(2,2), truncQ = 2, rm.phix = TRUE, compress = TRUE)
#look at the output. this tells you how many reads were removed
out
```

#Learn the error rates

```{r}
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)
```

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height =10, fig.width = 12, fig.align = "center", warning = FALSE}
plotErrors(errF, nominalQ = TRUE)

```

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height =10, fig.width = 12, fig.align = "center", warning = FALSE}
plotErrors(errR, nominalQ = TRUE)

```

#Dereplication

Dada2 combines all identical sequences into one unique sequence, keeping track of the number of identical sequences

```{r}
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```
# Infer the sequence variants
Apply the core dada2 sample inference algorithm to the dereplicated data.

Infer the sequence variants in each sample, taking out the sequence variants that have excessive error rates.

```{r}
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)
```

merge the overlapping reads -> this will also decrease the number of sequence variants.

```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE, trimOverhang = T)
```

```{r}
head(mergers[[1]])
```

```{r}
saveRDS(mergers, "~/GITHUB/144l_students/Output_Data/week5/dada_merged2.rds")
```

construct a sequence table of our samples that is analagous to the "OTU table" produced by classical methods
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab) #samples by unique sequences
```

check the distribution of the sequence lengths

```{r}
table(nchar(getSequences(seqtab)))
```

# Remove the Chimeras

in PCR, two or more biological sequences can attach to each other and then polymerase builds a non-biological sequence. Weird. These are are artifacts that need to be removed

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, verbose = TRUE)
dim(seqtab.nochim)
```

check the proportion of sequences that are not chimeras

```{r}
sum(seqtab.nochim)/sum(seqtab)
```

# Assign taxonomy using a reference database

here we are referencing the Silva database

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/GITHUB/144l_students/Input_Data/week5/Reference_Database/silva_nr_v138_train_set.fa", multithread = TRUE)
```

Create a table out of the taxa data (one with the sequences and assignments, one with just all the taxa)


